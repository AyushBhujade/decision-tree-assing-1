{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1136fb5d-671c-4f22-b034-59ca7b0e6933",
   "metadata": {},
   "source": [
    "# Quation 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aae3592-50a0-4ba8-a84e-6d04094bdb55",
   "metadata": {},
   "source": [
    "decision tree classifer is a supervised machine learning algorithm used for both classification and regression tasks.\n",
    "\n",
    "Node Splitting:\n",
    "              1) At the root node the algorithm select the attribute that best split the data based on the chosen criterion\n",
    "              2) The dataset is then divided into subset based on the value of the selected attribute\n",
    "\n",
    "Recursive Splitting:\n",
    "                   1) The process is  repeated for each subset,creating a tree structure\n",
    "                   2) At each internal node,the algorithm selects the best attribute to split the data \n",
    "                   3) The node splitting continues until having a minimum number of samples in a node\n",
    "                   \n",
    "Leaf Nodes:\n",
    "          1) Terminal node or leaf node represent the final decision or predicton.\n",
    "          2) Each leaf is associated with the majority class for classification or a predicted value for regression based on the sample\n",
    "             in that node\n",
    "\n",
    "Prediction:\n",
    "          1) To make prediction for a new instance,it traverses the tree from the root to a leaf based on the attribute value of the instance.\n",
    "          2) The prediction class or value is determined by the majority class or average value of the sample in the leaf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2135bcfa-cd30-4ab9-aa39-b31041c668b2",
   "metadata": {},
   "source": [
    "# Quation 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d828326d-4dca-45e5-80fa-0e1f4acd8bc1",
   "metadata": {},
   "source": [
    "Entropy:\n",
    "       1) Entropy measure the impurity or disorder in a set of data\n",
    "       2) for binary classification problem the entropy of a set is given by:\n",
    "          Entropy(S)=-p log2(p)-(1-p)log2(1-p)\n",
    "Information Gain:\n",
    "                1) information gain is used to determine te best attribute for splitting the data.\n",
    "                2) the algorithm select the attribute with the highest information gain or the lowest gini impurity\n",
    "                   to split the data at each node.\n",
    "Recursive Splitting:\n",
    "                    the chosen attribute is used to split the dataset into subsets and the process is repeat cursively for each subset\n",
    "Leaf nodes:\n",
    "            each leaf node is assigned the majority class in corresponding subset.\n",
    "Prediction:\n",
    "          1) To make prediction for a new instance,it traverses the tree from the root to a leaf based on the attribute value of the instance.\n",
    "          2) The prediction class or value is determined by the majority class or average value of the sample in the leaf.           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bf0719-a69b-437d-bdf5-2e9a2a65f95b",
   "metadata": {},
   "source": [
    "# Quation 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a86fbd-3d9e-4f6a-859a-7c678e6ecbad",
   "metadata": {},
   "source": [
    "Training:\n",
    "Given a labeled dataset with features and corresponding binary labels (e.g., 0 or 1), the decision tree algorithm learns to create a tree structure that best separates the classes.\n",
    "\n",
    "Node Splitting:\n",
    "At each node of the tree, the algorithm selects the feature that provides the best split, maximizing information gain or minimizing Gini impurity.\n",
    "The data is divided into subsets based on the chosen feature, creating branches in the tree.\n",
    "\n",
    "Recursive Splitting:\n",
    "The process of selecting features and splitting continues recursively until a stopping criterion is met (e.g., reaching a certain depth or having a minimum number of samples in a node).\n",
    "\n",
    "Leaf Nodes:\n",
    "The terminal nodes of the tree, called leaf nodes, represent the final decision or prediction.\n",
    "Each leaf node is associated with the majority class of the samples in that node.\n",
    "\n",
    "Prediction:\n",
    "To classify a new instance, the decision tree is traversed from the root to a leaf based on the on the attribute value of the instance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65690912-2126-4de9-b5cf-c41e0703f9c4",
   "metadata": {},
   "source": [
    "# Quation 4  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592ed31e-0351-4afe-847a-e128caee3f82",
   "metadata": {},
   "source": [
    "Feature Space Division:\n",
    "Imagine a multi-dimensional feature space where each dimension represents a feature of the data.\n",
    "Decision tree nodes correspond to splits along these dimensions, dividing the feature space into regions.\n",
    "\n",
    "Decision Boundaries:\n",
    "At each node, the decision tree algorithm selects the feature and threshold that best separates the data.\n",
    "The decision boundaries are perpendicular to the axes and align with the selected feature.\n",
    "\n",
    "Leaf Nodes:\n",
    "The terminal nodes, or leaf nodes, represent the final decision regions.\n",
    "Each leaf node is associated with a class, and the decision boundary leading to that leaf defines the conditions for belonging to that class.\n",
    "\n",
    "Predictions:\n",
    "To classify a new instance, you start at the root and traverse the tree based on the feature values of the instance.\n",
    "The path taken leads to a specific leaf node, determining the predicted class for the input.\n",
    "\n",
    "Visualization:\n",
    "if you ware to visualize the decision tree in 2D the decision boundaries would be lines. in 3D they would be planes,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de91df1-ce9e-4d38-baf8-59454d42558f",
   "metadata": {},
   "source": [
    "# Quation 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d762a36-421b-4e97-8092-e4e430417750",
   "metadata": {},
   "source": [
    "a confusion matrix is a table tha summarizes the performance of a classification model by comparing the predicted and the \n",
    "actual class labels. it provides a detailed breakdown of the model performance,especially in binary classification but can be extended\n",
    "to a multiclass problem.\n",
    "\n",
    "True Positive (TP):\n",
    "instances that are actually positive and predicted as positive\n",
    "\n",
    "True Negative (TN):\n",
    "instances that are actually negative and predicted as negative\n",
    "\n",
    "False Positive (FP):\n",
    "instances that are actully negative and predicted as positive.\n",
    "\n",
    "False negative (FN):\n",
    "instanses that are actually positive but predicted as negative \n",
    "\n",
    "Accuracy=\n",
    "   TP+TN/TP+TN+FP+FN\n",
    "   \n",
    "Precision=\n",
    "  TP/TP+FP\n",
    "  \n",
    "Recall=\n",
    "   TP/TP+FN\n",
    "\n",
    "F1Score= 2*Precision*recall/Precision+Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9505e38-ffa6-438b-a42f-dc0917942a8e",
   "metadata": {},
   "source": [
    "# Quation 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba422356-9aa1-4101-a66e-1f80d0911ef8",
   "metadata": {},
   "source": [
    "lets consider binary classification problem such as predicting whether an email is spam or not spam\n",
    "\n",
    "                    Actual Psitive  Actual negative \n",
    "\n",
    "predicted positive |      15               5     |\n",
    "\n",
    "Predicted negative |      2                78     |\n",
    " \n",
    " True Positive :\n",
    "                 15 email were actually spam and predicted as spam\n",
    " \n",
    " True Negative::\n",
    "                 78 email were actually not spam and predicted as not spam\n",
    "                 \n",
    " False Positive:\n",
    "                 5 email were actualy not spam but wrongly predicted as spam\n",
    "                 \n",
    " False negative:\n",
    "                2 email were actually spam but wrongly predicted as not spam\n",
    "\n",
    "Precision= TP/TP+FP\n",
    "          =  15/15+5\n",
    "          =15/20 =0.75\n",
    "\n",
    "Recall=TP/TP+FN\n",
    "        15/15+2\n",
    "        15/17=0.882\n",
    "\n",
    "F1 score=2* precision*Recall/precision+recall\n",
    "\n",
    "         =2*0.75*0.88/0.75+0.88\n",
    "         =0.81\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d82c33a-8c50-4d8d-b6a1-45f63bfd491c",
   "metadata": {},
   "source": [
    "# Quation 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab262c6-e189-438a-ba93-42bfba9a9f6b",
   "metadata": {},
   "source": [
    "Accuracy:\n",
    "Use Case: Suitable for balanced datasets.\n",
    "Consideration: May be misleading in imbalanced datasets, where a high accuracy can be achieved by simply predicting the majority class.\n",
    "\n",
    "Precision (Positive Predictive Value):\n",
    "Use Case: Important when the cost of false positives is high (e.g., in medical diagnoses).\n",
    "\n",
    "Calculation: \n",
    "\n",
    "Precision= \n",
    "TP + FP/TP\n",
    "\n",
    " \n",
    "Recall (Sensitivity, True Positive Rate):\n",
    "Use Case: Important when the cost of false negatives is high (e.g., in fraud detection).\n",
    "Calculation: \n",
    "Recall=TP/TP + FN\n",
    "\n",
    "F1 Score:\n",
    "Use Case: Balances precision and recall, suitable when there is an uneven class distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cb2027-db8b-43e2-8aeb-92304afcf1b3",
   "metadata": {},
   "source": [
    "# Quation 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8412ab-f4d7-4d1c-8e0d-1fd9515b0fd9",
   "metadata": {},
   "source": [
    "Consider a medical diagnostic system that identifies whether a patient has a rare but severe disease. In this scenario, precision becomes a crucial metric. \n",
    "Here's why:\n",
    "Example: Early Detection of a Rare Disease\n",
    "\n",
    "Positive Class (Presence of the Disease):\n",
    "\n",
    "True Positive (TP): The model correctly identifies a patient with the disease.\n",
    "False Positive (FP): The model incorrectly flags a patient as having the disease when they do not.\n",
    "Negative Class (Absence of the Disease):\n",
    "True Negative (TN): The model correctly identifies a patient without the disease.\n",
    "False Negative (FN): The model incorrectly predicts that a patient does not have the disease when they actually do.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b151b02-072f-4289-86d7-85da4341e812",
   "metadata": {},
   "source": [
    "# Quation 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d11dfb-71f2-4784-87ff-e5204fc67e86",
   "metadata": {},
   "source": [
    "Consider a fraud detection system in a financial transaction environment. In this scenario, recall becomes a crucial metric. Here's why:\n",
    "\n",
    "Example: Detecting Fraudulent Transactions\n",
    "\n",
    "Positive Class (Fraudulent Transactions):\n",
    "\n",
    "True Positive (TP): The model correctly identifies a transaction as fraudulent.\n",
    "False Negative (FN): The model fails to detect a fraudulent transaction.\n",
    "Negative Class (Non-Fraudulent Transactions):\n",
    "\n",
    "True Negative (TN): The model correctly identifies a transaction as non-fraudulent.\n",
    "False Positive (FP): The model incorrectly flags a non-fraudulent transaction as fraudulent.\n",
    "Importance of Recall:\n",
    "\n",
    "In the context of fraud detection:\n",
    "\n",
    "Recall: \n",
    "Recall= TP + FN/TP\n",
    "\n",
    " \n",
    "\n",
    "A high recall means that the model effectively captures most of the actual fraudulent transactions.\n",
    "False negatives, in this case, represent undetected fraud, which can lead to financial losses and damage trust.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
